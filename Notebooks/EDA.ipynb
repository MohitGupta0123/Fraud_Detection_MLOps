{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a3ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Standard Library --------------------\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------- Core Scientific Libraries --------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Visualization --------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------- Statistical Analysis --------------------\n",
    "from scipy.stats import skew, shapiro, kurtosis, chi2_contingency\n",
    "# -------------------- Scikit-learn Core --------------------\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7cf4636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accountAgeDays</th>\n",
       "      <th>numItems</th>\n",
       "      <th>localTime</th>\n",
       "      <th>paymentMethod</th>\n",
       "      <th>paymentMethodAgeDays</th>\n",
       "      <th>Category</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>4.745402</td>\n",
       "      <td>paypal</td>\n",
       "      <td>28.204861</td>\n",
       "      <td>shopping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>725</td>\n",
       "      <td>1</td>\n",
       "      <td>4.742303</td>\n",
       "      <td>storecredit</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>electronics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>845</td>\n",
       "      <td>1</td>\n",
       "      <td>4.921318</td>\n",
       "      <td>creditcard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>food</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>503</td>\n",
       "      <td>1</td>\n",
       "      <td>4.886641</td>\n",
       "      <td>creditcard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>electronics</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>5.040929</td>\n",
       "      <td>creditcard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>shopping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accountAgeDays  numItems  localTime paymentMethod  paymentMethodAgeDays  \\\n",
       "0              29         1   4.745402        paypal             28.204861   \n",
       "1             725         1   4.742303   storecredit              0.000000   \n",
       "2             845         1   4.921318    creditcard              0.000000   \n",
       "3             503         1   4.886641    creditcard              0.000000   \n",
       "4            2000         1   5.040929    creditcard              0.000000   \n",
       "\n",
       "      Category  isWeekend  label  \n",
       "0     shopping        0.0      0  \n",
       "1  electronics        0.0      0  \n",
       "2         food        1.0      0  \n",
       "3  electronics        1.0      0  \n",
       "4     shopping        0.0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Asus\\Downloads\\Fraud_MLOps_Project\\Data\\payment_fraud.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bcf5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dtale.show(df, ignore_duplicate=False)\n",
    "# d.open_browser()\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ab95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA:\n",
    "    def __init__(self, df, report_path=\"eda_results\", outlier_method=\"iqr\", threshold=1.5):\n",
    "        \"\"\"\n",
    "        outlier_method: 'iqr' or 'zscore'\n",
    "        threshold: For IQR -> multiplier (1.5), for Z-Score -> sigma (3)\n",
    "        \"\"\"\n",
    "        self.df = df.copy()  # Read-only\n",
    "        self.report_path = report_path\n",
    "        self.outlier_method = outlier_method\n",
    "        self.threshold = threshold\n",
    "        self.target_col = None\n",
    "\n",
    "        # Folder structure\n",
    "        folders = [\n",
    "            \"missing_data\", \"cardinality\", \"numerical_analysis\", \"categorical_analysis\",\n",
    "            \"relationships\", \"outliers\", \"distribution_shape\", \"target_analysis\",\n",
    "            \"advanced_relationships\"\n",
    "        ]\n",
    "        for folder in folders:\n",
    "            os.makedirs(os.path.join(report_path, folder), exist_ok=True)\n",
    "\n",
    "    # ----------------- Helper Functions -----------------\n",
    "    def save_text(self, folder, filename, content):\n",
    "        with open(os.path.join(self.report_path, folder, filename), \"w\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "    def set_target(self, target_name=None):\n",
    "        if target_name is None:\n",
    "            target_name = input(\"Enter target column name (or leave blank if none): \").strip()\n",
    "        if target_name and target_name in self.df.columns:\n",
    "            self.target_col = target_name\n",
    "            print(f\"[Info] Target column set to: {self.target_col}\")\n",
    "        else:\n",
    "            self.target_col = None\n",
    "            print(\"[Info] No target column set\")\n",
    "\n",
    "    # ----------------- 1. Missing Data Analysis -----------------\n",
    "    def advanced_missing_data(self):\n",
    "        missing = self.df.isnull().sum()\n",
    "        missing_percent = (missing / len(self.df)) * 100\n",
    "        summary = pd.DataFrame({\"Missing Count\": missing, \"Missing %\": missing_percent})\n",
    "        summary = summary[summary[\"Missing Count\"] > 0].sort_values(by=\"Missing %\", ascending=False)\n",
    "        self.save_text(\"missing_data\", \"missing_summary.txt\", str(summary))\n",
    "\n",
    "        # Heatmap\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(self.df.isnull(), cbar=False)\n",
    "        plt.title(\"Missing Data Heatmap\")\n",
    "        plt.savefig(os.path.join(self.report_path, \"missing_data\", \"missing_heatmap.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Missing correlation (binary mask correlation)\n",
    "        missing_corr = self.df.isnull().astype(int).corr()\n",
    "        self.save_text(\"missing_data\", \"missing_correlation.txt\", str(missing_corr))\n",
    "\n",
    "        print(\"[Saved] Missing data summary, heatmap & correlation\")\n",
    "\n",
    "    # ----------------- 2. Cardinality Analysis -----------------\n",
    "    def cardinality_analysis(self):\n",
    "        cat_cols = self.df.select_dtypes(exclude=np.number).columns\n",
    "        summary = {}\n",
    "        high_cardinality = []\n",
    "        for col in cat_cols:\n",
    "            count = self.df[col].nunique()\n",
    "            summary[col] = count\n",
    "            if count > 50:  # Arbitrary threshold\n",
    "                high_cardinality.append(col)\n",
    "        text = f\"Cardinality per categorical feature:\\n{summary}\\n\\nHigh cardinality features (>50 unique): {high_cardinality}\"\n",
    "        self.save_text(\"cardinality\", \"cardinality_summary.txt\", text)\n",
    "        print(\"[Saved] Cardinality summary with high-cardinality flags\")\n",
    "\n",
    "    # ----------------- 3. Numerical Feature Relationships -----------------\n",
    "    def numerical_relationships(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        if len(num_cols) <= 10 and len(num_cols) > 1:\n",
    "            sns.pairplot(self.df[num_cols].dropna())\n",
    "            plt.savefig(os.path.join(self.report_path, \"relationships\", \"pairplot.png\"))\n",
    "            plt.close()\n",
    "            print(\"[Saved] Pairplot for numerical features\")\n",
    "\n",
    "        # Boxplot of numeric vs categorical target\n",
    "        if self.target_col and self.df[self.target_col].dtype == 'object':\n",
    "            for col in num_cols:\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                sns.boxplot(x=self.df[self.target_col], y=self.df[col])\n",
    "                plt.title(f\"{col} vs {self.target_col} (Boxplot)\")\n",
    "                plt.savefig(os.path.join(self.report_path, \"relationships\", f\"{col}_vs_target_box.png\"))\n",
    "                plt.close()\n",
    "\n",
    "        # Scatterplot vs continuous target\n",
    "        if self.target_col and self.df[self.target_col].dtype != 'object':\n",
    "            for col in num_cols:\n",
    "                if col == self.target_col: continue\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                sns.scatterplot(x=self.df[col], y=self.df[self.target_col])\n",
    "                plt.title(f\"{col} vs {self.target_col}\")\n",
    "                plt.savefig(os.path.join(self.report_path, \"relationships\", f\"{col}_vs_target.png\"))\n",
    "                plt.close()\n",
    "\n",
    "    # ----------------- 4. Categorical Feature Relationships -----------------\n",
    "    def categorical_relationships(self):\n",
    "        cat_cols = self.df.select_dtypes(exclude=np.number).columns\n",
    "        chi_summary = []\n",
    "        for col1, col2 in itertools.combinations(cat_cols, 2):\n",
    "            table = pd.crosstab(self.df[col1], self.df[col2])\n",
    "            chi2, p, _, _ = chi2_contingency(table)\n",
    "            chi_summary.append(f\"{col1} vs {col2} -> p={p:.4f}\")\n",
    "        self.save_text(\"relationships\", \"categorical_chi2.txt\", \"\\n\".join(chi_summary))\n",
    "\n",
    "        # Cramér’s V\n",
    "        cramer_results = []\n",
    "        for col1, col2 in itertools.combinations(cat_cols, 2):\n",
    "            table = pd.crosstab(self.df[col1], self.df[col2])\n",
    "            chi2 = chi2_contingency(table)[0]\n",
    "            n = table.sum().sum()\n",
    "            phi2 = chi2 / n\n",
    "            r, k = table.shape\n",
    "            phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "            rcorr = r - ((r-1)**2)/(n-1)\n",
    "            kcorr = k - ((k-1)**2)/(n-1)\n",
    "            cramer_v = np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "            cramer_results.append(f\"{col1} vs {col2} -> Cramér's V={cramer_v:.4f}\")\n",
    "        self.save_text(\"relationships\", \"categorical_cramers_v.txt\", \"\\n\".join(cramer_results))\n",
    "        print(\"[Saved] Chi-square and Cramér's V results for categorical features\")\n",
    "\n",
    "    # ----------------- 5. Feature Importance (Mutual Information) -----------------\n",
    "    def feature_importance(self):\n",
    "        if self.target_col:\n",
    "            X = self.df.drop(columns=[self.target_col]).select_dtypes(include=np.number).fillna(0)\n",
    "            y = self.df[self.target_col]\n",
    "            if y.dtype == 'object':\n",
    "                y = y.astype('category').cat.codes\n",
    "                scores = mutual_info_classif(X, y)\n",
    "            else:\n",
    "                scores = mutual_info_regression(X, y)\n",
    "            mi_scores = pd.Series(scores, index=X.columns).sort_values(ascending=False)\n",
    "            self.save_text(\"relationships\", \"mutual_information.txt\", str(mi_scores))\n",
    "            print(\"[Saved] Mutual Information scores\")\n",
    "\n",
    "    # ----------------- 6. Target Advanced Analysis -----------------\n",
    "    def target_advanced(self):\n",
    "        if self.target_col:\n",
    "            # Class imbalance\n",
    "            if self.df[self.target_col].dtype == 'object':\n",
    "                counts = self.df[self.target_col].value_counts(normalize=True) * 100\n",
    "                self.save_text(\"target_analysis\", \"class_imbalance.txt\", str(counts))\n",
    "            # Target mean per categorical\n",
    "            cat_cols = self.df.select_dtypes(exclude=np.number).columns\n",
    "            for col in cat_cols:\n",
    "                if col != self.target_col:\n",
    "                    mean_vals = self.df.groupby(col)[self.target_col].mean(numeric_only=True)\n",
    "                    self.save_text(\"target_analysis\", f\"{col}_target_mean.txt\", str(mean_vals))\n",
    "            print(\"[Saved] Target distribution & mean analysis per category\")\n",
    "\n",
    "    # ----------------- 7. Correlation Beyond Pearson -----------------\n",
    "    def advanced_correlations(self):\n",
    "        num_df = self.df.select_dtypes(include=np.number)\n",
    "        if num_df.shape[1] > 1:\n",
    "            # Compute correlations\n",
    "            spearman_corr = num_df.corr(method='spearman')\n",
    "            kendall_corr = num_df.corr(method='kendall')\n",
    "\n",
    "            # Save correlation matrices\n",
    "            self.save_text(\"advanced_relationships\", \"spearman_corr.txt\", str(spearman_corr))\n",
    "            self.save_text(\"advanced_relationships\", \"kendall_corr.txt\", str(kendall_corr))\n",
    "\n",
    "            # Clean correlation matrix for clustering (replace NaN with 0)\n",
    "            corr_matrix = num_df.corr().fillna(0)\n",
    "\n",
    "            try:\n",
    "                sns.clustermap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "                plt.savefig(os.path.join(self.report_path, \"advanced_relationships\", \"clustered_corr_heatmap.png\"))\n",
    "                plt.close()\n",
    "                print(\"[Saved] Spearman, Kendall correlations & clustered heatmap\")\n",
    "            except ValueError:\n",
    "                print(\"[Warning] Could not generate clustered heatmap (NaNs/Infs in data even after fill).\")\n",
    "        else:\n",
    "            print(\"[Info] Not enough numerical columns for advanced correlation analysis.\")\n",
    "\n",
    "    # ----------------- 8. Outlier Profiling -----------------\n",
    "    def outlier_profiling(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        results = []\n",
    "        for col in num_cols:\n",
    "            Q1, Q3 = self.df[col].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "            outliers = self.df[(self.df[col] < lower) | (self.df[col] > upper)]\n",
    "            results.append(f\"{col}: {len(outliers)} outliers ({(len(outliers)/len(self.df))*100:.2f}%)\")\n",
    "            # Boxplot\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.boxplot(x=self.df[col])\n",
    "            plt.title(f\"Boxplot: {col}\")\n",
    "            plt.savefig(os.path.join(self.report_path, \"outliers\", f\"box_{col}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # Multivariate outliers using Isolation Forest\n",
    "        iso = IsolationForest(contamination=0.05)\n",
    "        preds = iso.fit_predict(self.df[num_cols].fillna(0))\n",
    "        outlier_count = np.sum(preds == -1)\n",
    "        results.append(f\"Multivariate Outliers (IsolationForest): {outlier_count}\")\n",
    "\n",
    "        self.save_text(\"outliers\", \"outlier_profile.txt\", \"\\n\".join(results))\n",
    "        print(\"[Saved] Outlier profiles & multivariate outliers\")\n",
    "\n",
    "    # ----------------- 9. Distribution Shape -----------------\n",
    "    def distribution_shape(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        results = []\n",
    "        for col in num_cols:\n",
    "            data = self.df[col].dropna()\n",
    "            skew_val = skew(data)\n",
    "            kurt_val = kurtosis(data)\n",
    "            stat, p = shapiro(data.sample(min(5000, len(data))))\n",
    "            results.append(f\"{col}: Skew={skew_val:.2f}, Kurtosis={kurt_val:.2f}, Shapiro p={p:.4f}\")\n",
    "            # Histogram\n",
    "            sns.histplot(data, kde=True)\n",
    "            plt.title(f\"Distribution: {col}\")\n",
    "            plt.savefig(os.path.join(self.report_path, \"distribution_shape\", f\"dist_{col}.png\"))\n",
    "            plt.close()\n",
    "        self.save_text(\"distribution_shape\", \"distribution_stats.txt\", \"\\n\".join(results))\n",
    "        print(\"[Saved] Skewness, kurtosis & normality plots\")\n",
    "\n",
    "    # ----------------- 10. Dataset Integrity -----------------\n",
    "    def integrity_checks(self):\n",
    "        results = []\n",
    "        # Constant features\n",
    "        constant_cols = [col for col in self.df.columns if self.df[col].nunique() == 1]\n",
    "        if constant_cols:\n",
    "            results.append(f\"Constant columns: {constant_cols}\")\n",
    "\n",
    "        # Highly correlated pairs\n",
    "        num_df = self.df.select_dtypes(include=np.number)\n",
    "        corr_matrix = num_df.corr().abs()\n",
    "        high_corr = np.where((corr_matrix > 0.9) & (corr_matrix < 1))\n",
    "        high_corr_pairs = [(num_df.columns[i], num_df.columns[j]) for i, j in zip(*high_corr)]\n",
    "        if high_corr_pairs:\n",
    "            results.append(f\"Highly correlated pairs: {high_corr_pairs}\")\n",
    "\n",
    "        # Duplicate features\n",
    "        duplicate_features = []\n",
    "        for col1, col2 in itertools.combinations(self.df.columns, 2):\n",
    "            if self.df[col1].equals(self.df[col2]):\n",
    "                duplicate_features.append((col1, col2))\n",
    "        if duplicate_features:\n",
    "            results.append(f\"Duplicate features: {duplicate_features}\")\n",
    "\n",
    "        self.save_text(\"relationships\", \"integrity_checks.txt\", \"\\n\".join(results) if results else \"No issues found\")\n",
    "        print(\"[Saved] Integrity checks\")\n",
    "\n",
    "    # ----------------- Numerical & Categorical Plots -----------------\n",
    "    def numerical_analysis(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        for col in num_cols:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.histplot(self.df[col].dropna(), kde=True)\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plt.savefig(os.path.join(self.report_path, \"numerical_analysis\", f\"hist_{col}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.boxplot(x=self.df[col].dropna())\n",
    "            plt.title(f\"Boxplot of {col}\")\n",
    "            plt.savefig(os.path.join(self.report_path, \"numerical_analysis\", f\"box_{col}.png\"))\n",
    "            plt.close()\n",
    "        print(\"[Saved] Numerical feature plots\")\n",
    "\n",
    "    def categorical_analysis(self):\n",
    "        cat_cols = self.df.select_dtypes(exclude=np.number).columns\n",
    "        for col in cat_cols:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.countplot(y=self.df[col], order=self.df[col].value_counts().index)\n",
    "            plt.title(f\"Count Plot of {col}\")\n",
    "            plt.savefig(os.path.join(self.report_path, \"categorical_analysis\", f\"count_{col}.png\"))\n",
    "            plt.close()\n",
    "        print(\"[Saved] Categorical feature plots\")\n",
    "    \n",
    "    def duplicate_rows_detail(self):\n",
    "        duplicates = self.df[self.df.duplicated()]\n",
    "        if not duplicates.empty:\n",
    "            duplicates.to_csv(os.path.join(self.report_path, \"relationships\", \"duplicate_rows.csv\"), index=False)\n",
    "            print(f\"[Saved] Detailed duplicate rows ({len(duplicates)} rows)\")\n",
    "        else:\n",
    "            print(\"[Info] No duplicate rows found\")\n",
    "    \n",
    "    def data_imbalance_heatmap(self):\n",
    "        if self.target_col and self.df[self.target_col].dtype == 'object':\n",
    "            counts = self.df[self.target_col].value_counts(normalize=True) * 100\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.heatmap(counts.to_frame(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "            plt.title(f\"Class Imbalance Heatmap for {self.target_col}\")\n",
    "            plt.savefig(os.path.join(self.report_path, \"target_analysis\", \"imbalance_heatmap.png\"))\n",
    "            plt.close()\n",
    "            print(\"[Saved] Target imbalance heatmap\")\n",
    "        else:\n",
    "            print(\"[Info] Target imbalance heatmap skipped (target not categorical)\")\n",
    "\n",
    "    def scaling_transformation_suggestions(self):\n",
    "        num_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        suggestions = []\n",
    "\n",
    "        for col in num_cols:\n",
    "            data = self.df[col].dropna()\n",
    "            if len(data) == 0: \n",
    "                continue\n",
    "\n",
    "            # Skewness check\n",
    "            skew_val = skew(data)\n",
    "            suggestion = f\"{col}: Skew={skew_val:.2f} -> \"\n",
    "            if abs(skew_val) > 1:\n",
    "                suggestion += \"Highly skewed; consider Log or Box-Cox transform\"\n",
    "            elif abs(skew_val) > 0.5:\n",
    "                suggestion += \"Moderately skewed; consider mild transform\"\n",
    "            else:\n",
    "                suggestion += \"Fairly symmetric\"\n",
    "\n",
    "            # Scaling suggestion\n",
    "            if (data.max() - data.min()) > 1000:\n",
    "                suggestion += \" | Suggest MinMax Scaling (large range)\"\n",
    "            else:\n",
    "                suggestion += \" | Suggest Standard Scaling (normal range)\"\n",
    "\n",
    "            suggestions.append(suggestion)\n",
    "\n",
    "        self.save_text(\"numerical_analysis\", \"scaling_transformation_suggestions.txt\", \"\\n\".join(suggestions))\n",
    "        print(\"[Saved] Scaling and transformation suggestions\")\n",
    "\n",
    "    # ----------------- Run All -----------------\n",
    "    def run_all(self, target_name=None):\n",
    "        print(\"===== Starting EDA =====\")\n",
    "        self.set_target(target_name)\n",
    "        self.advanced_missing_data()\n",
    "        self.cardinality_analysis()\n",
    "        self.numerical_analysis()\n",
    "        self.categorical_analysis()\n",
    "        self.numerical_relationships()\n",
    "        self.categorical_relationships()\n",
    "        self.feature_importance()\n",
    "        self.target_advanced()\n",
    "        self.advanced_correlations()\n",
    "        self.outlier_profiling()\n",
    "        self.distribution_shape()\n",
    "        self.integrity_checks()\n",
    "        self.duplicate_rows_detail()\n",
    "        self.data_imbalance_heatmap()\n",
    "        self.scaling_transformation_suggestions()\n",
    "\n",
    "        summary = \"EDA Completed.\\nCheck folders for:\\n\"\n",
    "        summary += \"\\n\".join([\n",
    "            \"- missing_data\", \"- cardinality\", \"- numerical_analysis\",\n",
    "            \"- categorical_analysis\", \"- relationships\", \"- outliers\",\n",
    "            \"- distribution_shape\", \"- target_analysis\", \"- advanced_relationships\"\n",
    "        ])\n",
    "        self.save_text(\"\", \"summary.txt\", summary)\n",
    "        print(f\"\\nEDA completed! Full report at '{self.report_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ccb18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting EDA =====\n",
      "[Info] Target column set to: label\n",
      "[Saved] Missing data summary, heatmap & correlation\n",
      "[Saved] Cardinality summary with high-cardinality flags\n",
      "[Saved] Numerical feature plots\n",
      "[Saved] Categorical feature plots\n",
      "[Saved] Pairplot for numerical features\n",
      "[Saved] Chi-square and Cramér's V results for categorical features\n",
      "[Saved] Mutual Information scores\n",
      "[Saved] Target distribution & mean analysis per category\n",
      "[Saved] Spearman, Kendall correlations & clustered heatmap\n",
      "[Saved] Outlier profiles & multivariate outliers\n",
      "[Saved] Skewness, kurtosis & normality plots\n",
      "[Saved] Integrity checks\n",
      "[Saved] Detailed duplicate rows (3033 rows)\n",
      "[Info] Target imbalance heatmap skipped (target not categorical)\n",
      "[Saved] Scaling and transformation suggestions\n",
      "\n",
      "EDA completed! Full report at 'Advanced_EDA_Results'\n"
     ]
    }
   ],
   "source": [
    "eda = EDA(df, report_path=\"Advanced_EDA_Results\", outlier_method=\"zscore\", threshold=3)\n",
    "eda.run_all(target_name=\"label\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6d0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eda_collage(report_path=\"eda_results\", output_file=\"eda_summary_collage.png\", cols=3, image_size=(600, 400)):\n",
    "    \"\"\"\n",
    "    Combine all .png images from EDA folders into one collage\n",
    "    \"\"\"\n",
    "    # Find all images recursively\n",
    "    image_paths = glob.glob(os.path.join(report_path, \"**\", \"*.png\"), recursive=True)\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"No images found to combine.\")\n",
    "        return\n",
    "\n",
    "    # Resize all images to uniform size\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = img.resize(image_size)  # Resize for uniformity\n",
    "        images.append(img)\n",
    "\n",
    "    # Determine grid size\n",
    "    total_images = len(images)\n",
    "    rows = math.ceil(total_images / cols)\n",
    "    width = cols * image_size[0]\n",
    "    height = rows * image_size[1]\n",
    "\n",
    "    # Create blank canvas\n",
    "    collage = Image.new(\"RGB\", (width, height), color=(255, 255, 255))\n",
    "\n",
    "    # Paste images in grid\n",
    "    for idx, img in enumerate(images):\n",
    "        x = (idx % cols) * image_size[0]\n",
    "        y = (idx // cols) * image_size[1]\n",
    "        collage.paste(img, (x, y))\n",
    "\n",
    "    # Save collage\n",
    "    collage.save(os.path.join(report_path, output_file))\n",
    "    print(f\"[Saved] Collage created at: {os.path.join(report_path, output_file)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40be9212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] Collage created at: Advanced_EDA_Results\\eda_summary_collage.png\n"
     ]
    }
   ],
   "source": [
    "create_eda_collage(report_path=\"Advanced_EDA_Results\", cols=4, image_size=(500, 350))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe4a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_summary(report_path=\"eda_results\", output_file=\"eda_text_summary.txt\"):\n",
    "    \"\"\"\n",
    "    Combine all EDA text results into one summary file\n",
    "    \"\"\"\n",
    "    text_paths = glob.glob(os.path.join(report_path, \"**\", \"*.txt\"), recursive=True)\n",
    "    summary_content = []\n",
    "\n",
    "    for path in sorted(text_paths):\n",
    "        # Skip the final summary itself to avoid duplication\n",
    "        if output_file in path:\n",
    "            continue\n",
    "        relative_path = os.path.relpath(path, report_path)\n",
    "        summary_content.append(f\"\\n===== {relative_path} =====\\n\")\n",
    "        with open(path, \"r\") as f:\n",
    "            summary_content.append(f.read())\n",
    "\n",
    "    with open(os.path.join(report_path, output_file), \"w\") as f:\n",
    "        f.write(\"\\n\".join(summary_content))\n",
    "\n",
    "    print(f\"[Saved] Combined text summary at: {os.path.join(report_path, output_file)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d005e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] Combined text summary at: Advanced_EDA_Results\\eda_text_summary.txt\n"
     ]
    }
   ],
   "source": [
    "create_text_summary(report_path=\"Advanced_EDA_Results\", output_file=\"eda_text_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
